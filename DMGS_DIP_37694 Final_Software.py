# -*- coding: utf-8 -*-
"""Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MImNhLB9PLurl4ougqchiv5zHYk4Bs-u
"""

import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('/content/creditcard.csv')

# Print dataset info
print("Dataset loaded successfully.")
print(f"First few rows of the dataset:\n{df.head()}")
print(f"Dataset shape: {df.shape}")
print(f"Columns in the dataset: {df.columns.tolist()}")

# Simulated data for ETL performance metrics
etl_metrics = {
    'Data Extraction Time': 1.5,  # in seconds
    'Data Transformation Time': 2.0,  # in seconds
    'Data Loading Time': 1.0  # in seconds
}

# Plot Bar Chart
plt.figure(figsize=(10, 6))
plt.bar(etl_metrics.keys(), etl_metrics.values(), color=['blue', 'green', 'red'])
plt.title('Performance Metrics of ETL Processes')
plt.xlabel('ETL Process')
plt.ylabel('Time (Seconds)')
plt.ylim(0, 3)  # Adjust the limit for better visualization
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('/content/creditcard.csv')

# Print dataset info
print("Dataset loaded successfully.")
print(f"First few rows of the dataset:\n{df.head()}")
print(f"Dataset shape: {df.shape}")
print(f"Columns in the dataset: {df.columns.tolist()}")

# Simulated data for stress testing
load_conditions = ['Low Load', 'Moderate Load', 'High Load', 'Extreme Load']
response_times = [0.5, 1.0, 2.0, 3.5]  # in seconds

# Plot Line Chart
plt.figure(figsize=(10, 6))
plt.plot(load_conditions, response_times, marker='o', linestyle='-', color='purple')
plt.title('System Performance Under Stress Testing')
plt.xlabel('Load Condition')
plt.ylabel('Response Time (Seconds)')
plt.grid(True)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('/content/creditcard.csv')

# Print dataset info
print("Dataset loaded successfully.")
print(f"First few rows of the dataset:\n{df.head()}")
print(f"Dataset shape: {df.shape}")
print(f"Columns in the dataset: {df.columns.tolist()}")

# Simulated data for User Acceptance Testing results
user_testing_results = {
    'Pass': 80,
    'Fail': 10,
    'Pending': 10
}

# Plot Pie Chart
plt.figure(figsize=(8, 8))
plt.pie(user_testing_results.values(), labels=user_testing_results.keys(), autopct='%1.1f%%', startangle=140, colors=['lightgreen', 'lightcoral', 'lightgoldenrodyellow'])
plt.title('Results of User Acceptance Testing')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('/content/creditcard.csv')

# Print dataset info
print("Dataset loaded successfully.")
print(f"First few rows of the dataset:\n{df.head()}")
print(f"Dataset shape: {df.shape}")
print(f"Columns in the dataset: {df.columns.tolist()}")

# Plot Histogram
plt.figure(figsize=(12, 6))
sns.histplot(df['Amount'], bins=50, kde=True)
plt.title('Distribution of Transaction Amounts')
plt.xlabel('Amount')
plt.ylabel('Frequency')
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('/content/creditcard.csv')

# Print dataset info
print("Dataset loaded successfully.")
print(f"First few rows of the dataset:\n{df.head()}")
print(f"Dataset shape: {df.shape}")
print(f"Columns in the dataset: {df.columns.tolist()}")

# Simulate some missing values for illustration
df.loc[np.random.choice(df.index, size=50, replace=False), 'Amount'] = np.nan

# Plot Heatmap for Missing Values
plt.figure(figsize=(12, 6))
sns.heatmap(df.isnull(), cbar=False, cmap='viridis')
plt.title('Heatmap of Missing Values')
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('/content/creditcard.csv')

# Simulate real-time monitoring data
times = pd.date_range(start='2024-01-01', periods=100, freq='H')
values = np.random.randint(0, 100, size=100)
real_time_df = pd.DataFrame({'Time': times, 'Transaction_Count': values})
real_time_df.set_index('Time', inplace=True)

# Print simulated data info
print("Simulated real-time monitoring data:")
print(real_time_df.head())

# Plot Line Chart for Real-Time Monitoring
plt.figure(figsize=(12, 6))
plt.plot(real_time_df.index, real_time_df['Transaction_Count'], marker='o', linestyle='-')
plt.title('Simulated Real-Time Transaction Monitoring')
plt.xlabel('Time')
plt.ylabel('Number of Transactions')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Load the dataset into a DataFrame
df = pd.read_csv('/content/creditcard.csv')

# Simulate data if necessary
def simulate_data(df):
    # Ensure that the 'Amount' column has some non-zero values
    if df['Amount'].min() == 0:
        df['Amount'] = df['Amount'].replace(0, np.random.uniform(0.1, 500.0, size=(df['Amount'] == 0).sum()))

    # Ensure that 'Class' has at least some fraudulent transactions
    if df['Class'].sum() == 0:
        num_fraud = int(df.shape[0] * 0.05)  # Simulate 5% fraudulent transactions
        fraud_indices = np.random.choice(df.index, num_fraud, replace=False)
        df.loc[fraud_indices, 'Class'] = 1

    # Ensure there are some missing values
    if df.isnull().sum().sum() == 0:
        # Randomly insert some missing values
        for col in ['Amount', 'Time']:
            df.loc[np.random.choice(df.index, size=int(df.shape[0] * 0.05), replace=False), col] = np.nan

    return df

# Apply simulation
df = simulate_data(df)

# Display the first few rows of the dataset to understand its structure
print(df.head())

# Check for missing values and basic statistics
print(df.isnull().sum())
print(df.describe())

# Data Distribution Graphs
plt.figure(figsize=(18, 12))

# Histogram of Transaction Amounts
plt.subplot(2, 2, 1)
sns.histplot(df['Amount'].clip(lower=0), bins=100, kde=True)
plt.title('Distribution of Transaction Amounts')
plt.xlabel('Transaction Amount')
plt.ylabel('Frequency')

# Box Plot of Transaction Amounts
plt.subplot(2, 2, 2)
sns.boxplot(x=df['Amount'].clip(lower=0))
plt.title('Box Plot of Transaction Amounts')
plt.xlabel('Transaction Amount')

# Missing Values Visualization
plt.subplot(2, 2, 3)
missing_values = df.isnull()
sns.heatmap(missing_values, cbar=False, cmap='viridis', linewidths=0.5, linecolor='black')
plt.title('Heatmap of Missing Values')
plt.xlabel('Columns')
plt.ylabel('Rows')

# Time Series Analysis
plt.subplot(2, 2, 4)
if 'Time' in df.columns:
    # Convert 'Time' to a datetime object for visualization
    df['Time'] = pd.to_datetime(df['Time'], unit='s', origin='unix')

    # Aggregate data by day
    df['Date'] = df['Time'].dt.date
    daily_transactions = df.groupby('Date').size()

    daily_transactions.plot()
    plt.title('Number of Transactions Over Time')
    plt.xlabel('Date')
    plt.ylabel('Number of Transactions')
    plt.xticks(rotation=45)
    plt.grid(True)
else:
    plt.text(0.5, 0.5, 'Time column not found', horizontalalignment='center', verticalalignment='center', fontsize=16)

plt.tight_layout()
plt.show()

# Data Aggregation: Bar Chart of Total Transaction Amounts by Class
plt.figure(figsize=(10, 6))
transaction_totals = df.groupby('Class')['Amount'].sum()
transaction_totals.plot(kind='bar', color=['blue', 'red'])
plt.title('Total Transaction Amounts by Fraud Class')
plt.xlabel('Fraud Class')
plt.ylabel('Total Amount')
plt.xticks(ticks=[0, 1], labels=['Legitimate', 'Fraudulent'], rotation=0)
plt.grid(axis='y')
plt.show()

# Real-Time Processing Monitoring (Simulated example)
plt.figure(figsize=(12, 6))
if 'Date' in df.columns and 'Class' in df.columns:
    fraud_counts = df.groupby(['Date', 'Class']).size().unstack().fillna(0)
    fraud_counts.plot()
    plt.title('Real-Time Fraud Detection Monitoring')
    plt.xlabel('Date')
    plt.ylabel('Count')
    plt.xticks(rotation=45)
    plt.grid(True)
else:
    plt.text(0.5, 0.5, 'Simulated Data for Real-Time Monitoring', horizontalalignment='center', verticalalignment='center', fontsize=16)
plt.show()
# Data Transformation Overview: Pair Plot
plt.figure(figsize=(14, 14))
selected_features = ['Amount', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']
if all(feature in df.columns for feature in selected_features):
    sns.pairplot(df[selected_features].dropna())
    plt.suptitle('Pair Plot of Key Features', y=1.02)
else:
    plt.text(0.5, 0.5, 'Some Features Missing', horizontalalignment='center', verticalalignment='center', fontsize=16)
plt.show()

# Install necessary libraries (uncomment if needed)
# !pip install pandas scikit-learn matplotlib seaborn

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
from google.colab import files
uploaded = files.upload()

# Assuming the dataset is in the uploaded file
filename = list(uploaded.keys())[0]
df = pd.read_csv(filename)

# Display the first few rows of the dataframe
print(df.head())

# Data Preprocessing
# Handling missing values if any
df = df.dropna()

# Separating features and target variable
X = df.drop(['Class'], axis=1)
y = df['Class']

# Scaling the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Initialize and train the model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)

# Evaluation
print("Classification Report:")
print(classification_report(y_test, y_pred))

print("Confusion Matrix:")
cm = confusion_matrix(y_test, y_pred)

# Plotting Confusion Matrix
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Feature Importance
features = X.columns
importances = model.feature_importances_
indices = np.argsort(importances)[::-1]

# Plotting Feature Importances
plt.figure(figsize=(12, 8))
plt.title('Feature Importances')
plt.bar(range(X.shape[1]), importances[indices], align='center')
plt.xticks(range(X.shape[1]), features[indices], rotation=90)
plt.xlim([-1, X.shape[1]])
plt.show()

!pip install networkx matplotlib
import matplotlib.pyplot as plt
import networkx as nx

# Create a directed graph
G = nx.DiGraph()

# Define nodes (Optimization Techniques)
nodes = [
    "Indexing", "Partitioning", "Materialized Views", "Data Compression",
    "Data Caching", "Query Optimization", "Load Balancing", "Distributed Computing",
    "Maintenance Routines", "Scalability", "ETL Process", "Data Lifecycle Management",
    "Performance Monitoring"
]

# Add nodes to the graph
G.add_nodes_from(nodes)

# Define edges (relationships between Optimization Techniques)
edges = [
    ("Indexing", "Query Optimization"),
    ("Partitioning", "Query Optimization"),
    ("Materialized Views", "Query Optimization"),
    ("Data Compression", "Query Optimization"),
    ("Data Caching", "Query Optimization"),
    ("Query Optimization", "Performance Monitoring"),
    ("Load Balancing", "Distributed Computing"),
    ("Distributed Computing", "Scalability"),
    ("Scalability", "ETL Process"),
    ("ETL Process", "Data Lifecycle Management"),
    ("Data Lifecycle Management", "Performance Monitoring"),
    ("Performance Monitoring", "Maintenance Routines")
]

# Add edges to the graph
G.add_edges_from(edges)

# Define layout for the graph
pos = nx.spring_layout(G, seed=42)

# Draw the graph
plt.figure(figsize=(14, 12))
nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightgreen', font_size=10, font_weight='bold', edge_color='black', linewidths=1, arrows=True, arrowsize=20)

# Set the title and display the plot
plt.title('Data Warehouse Optimization Strategies')
plt.show()

!pip install networkx matplotlib


import matplotlib.pyplot as plt
import networkx as nx

# Create a directed graph
G = nx.DiGraph()

# Define nodes (ETL Process Steps)
nodes = [
    "Start ETL Process",
    "Extract Data",
    "Transform Data",
    "Load Data",
    "Real-time Streaming Data",
    "Data Cleansing",
    "Data Normalization",
    "Data Enrichment",
    "Data Aggregation",
    "Data Validation",
    "Automated Scheduling & Monitoring",
    "End ETL Process"
]

# Add nodes to the graph
G.add_nodes_from(nodes)

# Define edges (relationships between ETL Process Steps)
edges = [
    ("Start ETL Process", "Extract Data"),
    ("Extract Data", "Real-time Streaming Data"),
    ("Extract Data", "Transform Data"),
    ("Transform Data", "Data Cleansing"),
    ("Data Cleansing", "Data Normalization"),
    ("Data Normalization", "Data Enrichment"),
    ("Data Enrichment", "Data Aggregation"),
    ("Data Aggregation", "Load Data"),
    ("Load Data", "Data Validation"),
    ("Data Validation", "Automated Scheduling & Monitoring"),
    ("Automated Scheduling & Monitoring", "End ETL Process")
]

# Add edges to the graph
G.add_edges_from(edges)

# Define layout for the graph
pos = nx.spring_layout(G, seed=42)

# Draw the graph
plt.figure(figsize=(14, 10))
nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', edge_color='gray', linewidths=1, arrows=True, arrowsize=20)

# Set the title and display the plot
plt.title('ETL Process Flowchart for Credit Card Fraud Detection')
plt.show()

# Install required libraries if not already installed
!pip install networkx matplotlib

import matplotlib.pyplot as plt
import networkx as nx

# Create a directed graph
G = nx.DiGraph()

# Define nodes (tables)
tables = [
    "Fact_Transactions", "Dim_Cards", "Dim_Merchants", "Dim_Customers",
    "Dim_Time", "Staging_Transactions", "Dim_Fraud_Alerts"
]

# Add nodes to the graph
G.add_nodes_from(tables)

# Define edges (relationships between tables)
edges = [
    ("Fact_Transactions", "Dim_Cards"),
    ("Fact_Transactions", "Dim_Merchants"),
    ("Fact_Transactions", "Dim_Customers"),
    ("Fact_Transactions", "Dim_Time"),
    ("Fact_Transactions", "Dim_Fraud_Alerts"),
    ("Staging_Transactions", "Fact_Transactions"),
    ("Staging_Transactions", "Dim_Cards"),
    ("Staging_Transactions", "Dim_Merchants"),
    ("Staging_Transactions", "Dim_Customers"),
    ("Staging_Transactions", "Dim_Time")
]

# Add edges to the graph
G.add_edges_from(edges)

# Define layout for the graph
pos = nx.spring_layout(G)

# Draw the graph
plt.figure(figsize=(12, 8))
nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', edge_color='gray', linewidths=1, arrows=True, arrowsize=20)

# Set the title and display the plot
plt.title('Schema Design for Credit Card Fraud Detection Data Warehouse')
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
import seaborn as sns

# Load the data
print("Step 1: Loading the dataset...")
df=pd.read_csv("/content/Book12.csv")
print("Dataset loaded successfully.")
print(f"First few rows of the dataset:\n{df.head()}")
print(df)

# Separate features and target variable
X = df.drop('Class', axis=1)
y = df['Class']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Apply scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Create and train the Random Forest model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Support Vector Machine
svm_model = SVC(random_state=42)
svm_model.fit(X_train, y_train)
y_pred_svm = svm_model.predict(X_test)

# Logistic Regression
lr_model = LogisticRegression(random_state=42, max_iter=1000)
lr_model.fit(X_train, y_train)
y_pred_lr = lr_model.predict(X_test)

# Random Forest Evaluation
print("Random Forest Evaluation:")
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("Classification Report:")
print(classification_report(y_test, y_pred))
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}\n")

# SVM Evaluation
print("SVM Evaluation:")
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_svm))
print("Classification Report:")
print(classification_report(y_test, y_pred_svm))
print(f"Accuracy: {accuracy_score(y_test, y_pred_svm):.4f}\n")

# Logistic Regression Evaluation
print("Logistic Regression Evaluation:")
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_lr))
print("Classification Report:")
print(classification_report(y_test, y_pred_lr))
print(f"Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\n")

import matplotlib.pyplot as plt

# Store accuracy scores
accuracies = {
    'Random Forest': accuracy_score(y_test, y_pred),
    'SVM': accuracy_score(y_test, y_pred_svm),
    'Logistic Regression': accuracy_score(y_test, y_pred_lr)
}

# Plotting
plt.bar(accuracies.keys(), accuracies.values())
plt.ylabel('Accuracy')
plt.title('Model Comparison')
plt.show()

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

# Random Forest
y_prob_rf = model.predict_proba(X_test)[:, 1]
fpr_rf, tpr_rf, _ = roc_curve(y_test, y_prob_rf)
roc_auc_rf = roc_auc_score(y_test, y_prob_rf)

# SVM (with probability=True to get probabilities)
svm_model = SVC(random_state=42, probability=True)
svm_model.fit(X_train, y_train)
y_prob_svm = svm_model.predict_proba(X_test)[:, 1]
fpr_svm, tpr_svm, _ = roc_curve(y_test, y_prob_svm)
roc_auc_svm = roc_auc_score(y_test, y_prob_svm)

# Logistic Regression
y_prob_lr = lr_model.predict_proba(X_test)[:, 1]
fpr_lr, tpr_lr, _ = roc_curve(y_test, y_prob_lr)
roc_auc_lr = roc_auc_score(y_test, y_prob_lr)

# Plotting ROC Curves
plt.figure()

plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {roc_auc_rf:.2f})')
plt.plot(fpr_svm, tpr_svm, label=f'SVM (AUC = {roc_auc_svm:.2f})')
plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {roc_auc_lr:.2f})')

plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curves')
plt.legend(loc="lower right")
plt.show()

# Generate the confusion matrix
cm = confusion_matrix(y_test, y_pred)
cm_df = pd.DataFrame(cm, index=['True Fraud', 'True No Fraud'], columns=['Pred Fraud', 'Pred No Fraud'])

# Evaluate the model
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Generate the classification report
report = classification_report(y_test, y_pred, target_names=['No Fraud', 'Fraud'], output_dict=True)

# Plot precision, recall, and F1-score for each class
plt.figure(figsize=(10, 7))
df_report = pd.DataFrame(report).transpose()
df_report.drop(columns=['support'], inplace=True)
df_report.plot(kind='bar', figsize=(12, 6))
plt.title('Classification Report')
plt.xlabel('Metrics')
plt.ylabel('Scores')
plt.xticks(rotation=45)
plt.legend(title='Class')
plt.show()